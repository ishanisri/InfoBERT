{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANLI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishanisri/InfoBERT/blob/master/ANLI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0T3t5WiY7gR",
        "outputId": "4226cbbc-b65b-4aa2-ad29-b77f8f25ae21"
      },
      "source": [
        "!git clone https://github.com/ishanisri/InfoBERT.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'InfoBERT'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (136/136), done.\u001b[K\n",
            "remote: Total 190 (delta 106), reused 128 (delta 49), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (190/190), 222.72 KiB | 8.91 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-txytkxiZKZS",
        "outputId": "aa39f56e-6c9b-49ca-9463-41d5f7da332f"
      },
      "source": [
        "%cd InfoBERT"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/InfoBERT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHrT-FnmyR_A",
        "outputId": "0889c13a-529b-41b2-aeca-9c86f60a3075"
      },
      "source": [
        "!git checkout texthide"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Branch 'texthide' set up to track remote branch 'texthide' from 'origin'.\n",
            "Switched to a new branch 'texthide'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B1OhUkg1cq0D",
        "outputId": "9b6b9782-e1b7-4c9e-ea8c-12bcff338e7a"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch~=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 21kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock~=3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.0.12)\n",
            "Collecting nlp~=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 48.3MB/s \n",
            "\u001b[?25hCollecting transformers~=2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 41.5MB/s \n",
            "\u001b[?25hCollecting numpy~=1.18.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.3MB/s \n",
            "\u001b[?25hCollecting scikit-learn~=0.21.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/c5/e5267eb84994e9a92a2c6a6ee768514f255d036f3c8378acfa694e9f2c99/scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging~=20.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (20.9)\n",
            "Collecting tqdm~=4.46.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.7MB/s \n",
            "\u001b[?25hCollecting tensorboardX~=1.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 39.2MB/s \n",
            "\u001b[?25hCollecting matplotlib~=3.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/9b/35ab3469fd1509f7636a344940569ebfd33239673fd2318e80b4700a257c/matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 49.5MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 71.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp~=0.4.0->-r requirements.txt (line 3)) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlp~=0.4.0->-r requirements.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from nlp~=0.4.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp~=0.4.0->-r requirements.txt (line 3)) (3.0.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 61.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0->-r requirements.txt (line 4)) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/59/bb06dd5ca53547d523422d32735585493e0103c992a52a97ba3aa3be33bf/tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.21.3->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.21.3->-r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging~=20.3->-r requirements.txt (line 7)) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX~=1.9->-r requirements.txt (line 9)) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX~=1.9->-r requirements.txt (line 9)) (3.12.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.1.3->-r requirements.txt (line 10)) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.1.3->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.1.3->-r requirements.txt (line 10)) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp~=0.4.0->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp~=0.4.0->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp~=0.4.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp~=0.4.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp~=0.4.0->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=2.11.0->-r requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX~=1.9->-r requirements.txt (line 9)) (54.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=828812659421594981b7abb087bbaa796cacae70ae443214345fde9ebca6209a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, xxhash, numpy, tqdm, nlp, sacremoses, sentencepiece, tokenizers, transformers, scikit-learn, tensorboardX, matplotlib\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed matplotlib-3.1.3 nlp-0.4.0 numpy-1.18.5 sacremoses-0.0.44 scikit-learn-0.21.3 sentencepiece-0.1.95 tensorboardX-1.9 tokenizers-0.7.0 torch-1.4.0 tqdm-4.46.1 transformers-2.11.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghbQeldMZOFY",
        "outputId": "840ae074-f80c-4969-e7c2-c7ae28a34a2f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXkAQ5Kc9Y55",
        "outputId": "f24f1694-0cbb-47bf-f2c0-7e72c6dba1ea"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UdGMGGo9-LEX",
        "outputId": "4e8d0408-c07b-43eb-d6df-07e5077497e0"
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive('day1ANLIfull1500', 'zip', 'InfoBERT/ANLI/infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-1500')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/day1ANLIfull1500.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xazo92T8d_y"
      },
      "source": [
        "!mv day1ANLIfull1500.zip   drive/MyDrive/ANLIfull/ANLIfull1500.zip"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqbAtgz6DPZP"
      },
      "source": [
        "rm -rf InfoBERT/ANLI/infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-1500"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8-rGB-daQru",
        "outputId": "8965a660-7038-414d-eea3-bfb9ee5d52cc"
      },
      "source": [
        "!unzip ../drive/MyDrive/anli_data/anli_data.zip -d ANLI"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ../drive/MyDrive/anli_data/anli_data.zip\n",
            "   creating: ANLI/anli_data/\n",
            "   creating: ANLI/anli_data/MNLI/\n",
            "  inflating: ANLI/anli_data/MNLI/README.txt  \n",
            "  inflating: ANLI/anli_data/MNLI/dev_matched.tsv  \n",
            "  inflating: ANLI/anli_data/MNLI/dev_mismatched.tsv  \n",
            "  inflating: ANLI/anli_data/MNLI/test_matched.tsv  \n",
            "  inflating: ANLI/anli_data/MNLI/test_mismatched.tsv  \n",
            "  inflating: ANLI/anli_data/MNLI/train.tsv  \n",
            "   creating: ANLI/anli_data/SNLI/\n",
            "  inflating: ANLI/anli_data/SNLI/README.txt  \n",
            "  inflating: ANLI/anli_data/SNLI/dev.tsv  \n",
            "  inflating: ANLI/anli_data/SNLI/test.tsv  \n",
            "  inflating: ANLI/anli_data/SNLI/train.tsv  \n",
            "   creating: ANLI/anli_data/infobert_mnli_matched/\n",
            "  inflating: ANLI/anli_data/infobert_mnli_matched/adversaries.txt  \n",
            "  inflating: ANLI/anli_data/infobert_mnli_matched/infobert_adv_dataset.jsonl  \n",
            "  inflating: ANLI/anli_data/infobert_mnli_matched/results_log  \n",
            "   creating: ANLI/anli_data/infobert_mnli_mismatched/\n",
            "  inflating: ANLI/anli_data/infobert_mnli_mismatched/adversaries.txt  \n",
            "  inflating: ANLI/anli_data/infobert_mnli_mismatched/infobert_adv_dataset.jsonl  \n",
            "  inflating: ANLI/anli_data/infobert_mnli_mismatched/results_log  \n",
            "   creating: ANLI/anli_data/infobert_snli/\n",
            "  inflating: ANLI/anli_data/infobert_snli/adversaries.txt  \n",
            "  inflating: ANLI/anli_data/infobert_snli/infobert_adv_dataset.jsonl  \n",
            "  inflating: ANLI/anli_data/infobert_snli/results_log  \n",
            "   creating: ANLI/anli_data/nli_fever/\n",
            "  inflating: ANLI/anli_data/nli_fever/README.md  \n",
            "  inflating: ANLI/anli_data/nli_fever/dev_fitems.jsonl  \n",
            "  inflating: ANLI/anli_data/nli_fever/test_fitems.jsonl  \n",
            "  inflating: ANLI/anli_data/nli_fever/train_fitems.jsonl  \n",
            "   creating: ANLI/anli_data/roberta_mnli_matched/\n",
            "  inflating: ANLI/anli_data/roberta_mnli_matched/adversaries.txt  \n",
            "  inflating: ANLI/anli_data/roberta_mnli_matched/results_log  \n",
            "  inflating: ANLI/anli_data/roberta_mnli_matched/roberta_adv_dataset.jsonl  \n",
            "   creating: ANLI/anli_data/roberta_mnli_mismatched/\n",
            "  inflating: ANLI/anli_data/roberta_mnli_mismatched/adversaries.txt  \n",
            "  inflating: ANLI/anli_data/roberta_mnli_mismatched/results_log  \n",
            "  inflating: ANLI/anli_data/roberta_mnli_mismatched/roberta_adv_dataset.jsonl  \n",
            "   creating: ANLI/anli_data/roberta_snli/\n",
            "  inflating: ANLI/anli_data/roberta_snli/adversaries.txt  \n",
            "  inflating: ANLI/anli_data/roberta_snli/results_log  \n",
            "  inflating: ANLI/anli_data/roberta_snli/roberta_adv_dataset.jsonl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S58L5j9H1nZ",
        "outputId": "eabd5743-e699-4b6b-ccab-363ee650c9d0"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mANLI\u001b[0m/  README.md  \u001b[01;34manli_data\u001b[0m/  requirements.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "355iXWuSnD-q",
        "outputId": "54b08994-73a2-497e-8fb7-fc32f4bb9688"
      },
      "source": [
        "cd ANLI\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/InfoBERT/ANLI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVMqtvhOoIJe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THPqEa_CzUc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a311937-76db-474a-a31d-35f5025df74c"
      },
      "source": [
        "!pip uninstall -y transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling transformers-2.11.0:\n",
            "  Successfully uninstalled transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JNlrWrsFD3t",
        "outputId": "94d537ce-24a9-441b-ca28-975a862a5949"
      },
      "source": [
        "cd InfoBERT/ANLI/"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/InfoBERT/ANLI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNAQGcG-eKjf",
        "outputId": "4f2d9ee9-63c4-4107-bee0-bbac9a569443"
      },
      "source": [
        "!source setup.sh && runexp      anli-full      infobert     roberta-large          2e-5      16           128              -1                1000          42           1e-5         5e-3            6       0.1    0       4e-2         8e-2        0        3         5e-3      0.5     0.9    4   256    \"texthide\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/eval_hist.bin exists.\n",
            "Resume Training from checkpoint infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500\n",
            "Master port: 6972\n",
            "2021-04-15 15:16:04.332121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/15/2021 15:16:06 - INFO - advtraining_args -   PyTorch: setting up devices\n",
            "04/15/2021 15:16:06 - INFO - advtraining_args -   256\n",
            "04/15/2021 15:16:06 - INFO - advtraining_args -   4\n",
            "04/15/2021 15:16:06 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n",
            "04/15/2021 15:16:06 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=1000, logging_dir='infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, alpha=0.005, adv_lr=0.04, adv_steps=3, adv_init_mag=0.08, norm_type='l2', adv_max_norm=0.0, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.0, cl=0.5, ch=0.9, num_k=256, num_sigma=4)\n",
            "04/15/2021 15:16:06 - INFO - __main__ -   256\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   in comfiggg dict\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   loading configuration file infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/config.json\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   kwargss\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   {'num_labels': 3, 'finetuning_task': 'anli-full', 'cache_dir': None, 'num_sigma': 4, 'num_k': 256, 'output_hidden_states': True, 'attention_probs_dropout_prob': 0.0, 'hidden_dropout_prob': 0.1}\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   Utilsss\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_auto -   RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   kwargss\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   {'num_labels': 3, 'finetuning_task': 'anli-full', 'cache_dir': None, 'num_sigma': 4, 'num_k': 256, 'output_hidden_states': True, 'attention_probs_dropout_prob': 0.0, 'hidden_dropout_prob': 0.1}\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   Utilsss\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - __main__ -   configgg from run\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_auto -   kwargs\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_auto -   {'cache_dir': None}\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   in comfiggg dict\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   loading configuration file infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/config.json\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   kwargss\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   {'cache_dir': None}\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   Utilsss\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_auto -   RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   kwargss\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   {'cache_dir': None}\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   Utilsss\n",
            "04/15/2021 15:16:06 - INFO - models.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_auto -   tokenozationn\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_auto -   RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_utils_base -   Model name 'infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_utils_base -   Didn't find file infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/added_tokens.json. We won't load it.\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_utils_base -   Didn't find file infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/tokenizer.json. We won't load it.\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_utils_base -   loading file infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/vocab.json\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_utils_base -   loading file infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/merges.txt\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_utils_base -   loading file None\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_utils_base -   loading file infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/special_tokens_map.json\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_utils_base -   loading file infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/tokenizer_config.json\n",
            "04/15/2021 15:16:06 - INFO - models.tokenization_utils_base -   loading file None\n",
            "04/15/2021 15:16:06 - INFO - __main__ -   RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - models.modeling_auto -   Modelinggg config\n",
            "04/15/2021 15:16:06 - INFO - models.modeling_auto -   RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - models.modeling_auto -   checkkk\n",
            "04/15/2021 15:16:06 - INFO - models.modeling_auto -   <class 'models.roberta.RobertaForSequenceClassification'>\n",
            "04/15/2021 15:16:06 - INFO - models.modeling_utils -   modeling_utils\n",
            "04/15/2021 15:16:06 - INFO - models.modeling_utils -   RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/15/2021 15:16:06 - INFO - models.modeling_utils -   loading weights file infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500/pytorch_model.bin\n",
            "04/15/2021 15:16:06 - INFO - models.roberta -   configgg\n",
            "04/15/2021 15:16:06 - INFO - models.roberta -   RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"anli-full\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_k\": 256,\n",
            "  \"num_sigma\": 4,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "TextHide parameters: 4 256\n",
            "04/15/2021 15:16:16 - INFO - models.modeling_utils -   All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
            "\n",
            "04/15/2021 15:16:16 - INFO - models.modeling_utils -   All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
            "04/15/2021 15:16:18 - INFO - filelock -   Lock 140555428083984 acquired on anli_data/cached_train_RobertaTokenizer_128_anli-full.lock\n",
            "04/15/2021 15:16:44 - INFO - datasets.anli -   Loading features from cached file anli_data/cached_train_RobertaTokenizer_128_anli-full [took 25.380 s]\n",
            "04/15/2021 15:16:44 - INFO - filelock -   Lock 140555428083984 released on anli_data/cached_train_RobertaTokenizer_128_anli-full.lock\n",
            "04/15/2021 15:16:44 - INFO - filelock -   Lock 140555428099664 acquired on anli_data/cached_dev_RobertaTokenizer_128_anli-full.lock\n",
            "04/15/2021 15:16:44 - INFO - datasets.anli -   Loading features from cached file anli_data/cached_dev_RobertaTokenizer_128_anli-full [took 0.035 s]\n",
            "04/15/2021 15:16:44 - INFO - filelock -   Lock 140555428099664 released on anli_data/cached_dev_RobertaTokenizer_128_anli-full.lock\n",
            "04/15/2021 15:16:44 - INFO - __main__ -   Load mi estimator successful from infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-9500\n",
            "04/15/2021 15:16:45 - INFO - local_robust_trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -   ***** Running training *****\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Num examples = 1313280\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Num Epochs = 3\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Instantaneous batch size per device = 16\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Gradient Accumulation steps = 1\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Total optimization steps = 246240\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Continuing training from checkpoint, will skip to saved global_step\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Continuing training from epoch 0\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Continuing training from global step 9500\n",
            "04/15/2021 15:16:47 - INFO - local_robust_trainer -     Will skip the first 9500 steps in the first epoch\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/82080 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 105/82080 [00:00<01:18, 1049.81it/s]\u001b[A\n",
            "Iteration:   1% 989/82080 [00:00<00:56, 1427.08it/s]\u001b[A\n",
            "Iteration:   2% 1851/82080 [00:00<00:42, 1903.56it/s]\u001b[A\n",
            "Iteration:   3% 2730/82080 [00:00<00:31, 2488.25it/s]\u001b[A\n",
            "Iteration:   4% 3598/82080 [00:00<00:24, 3165.41it/s]\u001b[A\n",
            "Iteration:   5% 4478/82080 [00:00<00:19, 3917.40it/s]\u001b[A\n",
            "Iteration:   6% 5301/82080 [00:00<00:16, 4647.42it/s]\u001b[A\n",
            "Iteration:   7% 6137/82080 [00:00<00:14, 5360.87it/s]\u001b[A\n",
            "Iteration:   8% 6932/82080 [00:00<00:12, 5940.52it/s]\u001b[A\n",
            "Iteration:   9% 7770/82080 [00:01<00:11, 6507.95it/s]\u001b[A\n",
            "Iteration:  11% 8637/82080 [00:01<00:10, 7033.32it/s]\u001b[A\n",
            "Iteration:  12% 9487/82080 [00:01<00:09, 7415.17it/s]\u001b[A\n",
            "Iteration:  12% 9487/82080 [00:20<00:09, 7415.17it/s]\u001b[A\n",
            "Iteration:  12% 9510/82080 [00:21<5:20:02,  3.78it/s]\u001b[A\n",
            "Iteration:  12% 9511/82080 [00:23<16:24:47,  1.23it/s]\u001b[A\n",
            "Iteration:  12% 9512/82080 [00:25<23:44:17,  1.18s/it]\u001b[A\n",
            "Iteration:  12% 9513/82080 [00:27<28:54:01,  1.43s/it]\u001b[A\n",
            "Iteration:  12% 9514/82080 [00:29<32:14:48,  1.60s/it]\u001b[A\n",
            "Iteration:  12% 9515/82080 [00:31<35:12:30,  1.75s/it]\u001b[A\n",
            "Iteration:  12% 9516/82080 [00:33<36:39:39,  1.82s/it]\u001b[A\n",
            "Iteration:  12% 9517/82080 [00:35<38:06:08,  1.89s/it]\u001b[A\n",
            "Iteration:  12% 9518/82080 [00:37<38:55:59,  1.93s/it]\u001b[A\n",
            "Iteration:  12% 9519/82080 [00:39<39:55:41,  1.98s/it]\u001b[A\n",
            "Iteration:  12% 9520/82080 [00:41<40:15:01,  2.00s/it]\u001b[A\n",
            "Iteration:  12% 9521/82080 [00:43<40:08:50,  1.99s/it]\u001b[A\n",
            "Iteration:  12% 9522/82080 [00:46<40:51:09,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9523/82080 [00:48<40:52:59,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9524/82080 [00:50<40:48:11,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9525/82080 [00:52<41:00:52,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9526/82080 [00:54<40:44:27,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9527/82080 [00:56<40:42:08,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9528/82080 [00:58<40:41:43,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9529/82080 [01:00<40:59:40,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9530/82080 [01:02<41:09:07,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9531/82080 [01:04<41:06:43,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9532/82080 [01:06<40:48:40,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9533/82080 [01:08<40:40:45,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9534/82080 [01:10<40:22:33,  2.00s/it]\u001b[A\n",
            "Iteration:  12% 9535/82080 [01:12<40:30:21,  2.01s/it]\u001b[A\n",
            "Iteration:  12% 9536/82080 [01:14<40:14:30,  2.00s/it]\u001b[A\n",
            "Iteration:  12% 9537/82080 [01:16<40:20:44,  2.00s/it]\u001b[A\n",
            "Iteration:  12% 9538/82080 [01:18<40:22:46,  2.00s/it]\u001b[A\n",
            "Iteration:  12% 9539/82080 [01:20<40:42:39,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9540/82080 [01:22<40:53:53,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9541/82080 [01:24<40:48:07,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9542/82080 [01:26<40:54:01,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9543/82080 [01:28<40:53:23,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9544/82080 [01:30<41:07:48,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9545/82080 [01:32<41:11:40,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9546/82080 [01:34<41:00:56,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9547/82080 [01:36<41:11:27,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9548/82080 [01:38<41:44:01,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9549/82080 [01:40<41:21:12,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9550/82080 [01:42<40:42:18,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9551/82080 [01:44<40:32:45,  2.01s/it]\u001b[A\n",
            "Iteration:  12% 9552/82080 [01:46<40:43:47,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9553/82080 [01:48<41:06:00,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9554/82080 [01:50<40:59:12,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9555/82080 [01:52<40:57:06,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9556/82080 [01:55<41:14:12,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9557/82080 [01:57<41:06:06,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9558/82080 [01:59<41:10:31,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9559/82080 [02:01<41:22:46,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9560/82080 [02:03<41:15:22,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9561/82080 [02:05<41:12:02,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9562/82080 [02:07<41:04:39,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9563/82080 [02:09<41:26:04,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9564/82080 [02:11<41:10:24,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9565/82080 [02:13<41:10:49,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9566/82080 [02:15<41:18:53,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9567/82080 [02:17<41:15:21,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9568/82080 [02:19<41:33:55,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9569/82080 [02:21<41:32:56,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9570/82080 [02:23<41:45:46,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9571/82080 [02:25<41:14:20,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9572/82080 [02:27<41:14:25,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9573/82080 [02:29<40:50:27,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9574/82080 [02:31<40:37:26,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9575/82080 [02:33<41:05:03,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9576/82080 [02:35<41:12:42,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9577/82080 [02:37<41:07:49,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9578/82080 [02:39<40:40:09,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9579/82080 [02:42<41:25:13,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9580/82080 [02:44<41:30:41,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9581/82080 [02:46<41:03:26,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9582/82080 [02:48<41:34:57,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9583/82080 [02:50<41:23:23,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9584/82080 [02:52<41:55:46,  2.08s/it]\u001b[A\n",
            "Iteration:  12% 9585/82080 [02:54<41:26:47,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9586/82080 [02:56<41:22:21,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9587/82080 [02:58<41:16:15,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9588/82080 [03:00<41:23:08,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9589/82080 [03:02<41:31:44,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9590/82080 [03:04<41:47:56,  2.08s/it]\u001b[A\n",
            "Iteration:  12% 9591/82080 [03:06<41:41:20,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9592/82080 [03:08<41:42:51,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9593/82080 [03:10<41:31:28,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9594/82080 [03:12<41:19:40,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9595/82080 [03:15<41:05:44,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9596/82080 [03:17<41:26:09,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9597/82080 [03:19<40:58:43,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9598/82080 [03:21<41:17:17,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9599/82080 [03:23<41:09:04,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9600/82080 [03:25<41:21:59,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9601/82080 [03:27<41:02:43,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9602/82080 [03:29<40:42:20,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9603/82080 [03:31<40:54:52,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9604/82080 [03:33<40:43:47,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9605/82080 [03:35<40:54:30,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9606/82080 [03:37<40:52:56,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9607/82080 [03:39<41:01:39,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9608/82080 [03:41<40:50:17,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9609/82080 [03:43<40:42:21,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9610/82080 [03:45<40:54:59,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9611/82080 [03:47<41:18:52,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9612/82080 [03:49<40:55:54,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9613/82080 [03:51<41:35:19,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9614/82080 [03:53<41:34:56,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9615/82080 [03:55<41:18:05,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9616/82080 [03:57<41:25:45,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9617/82080 [03:59<41:16:49,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9618/82080 [04:01<40:52:03,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9619/82080 [04:03<40:37:30,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9620/82080 [04:05<40:39:04,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9621/82080 [04:07<40:42:28,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9622/82080 [04:10<40:44:21,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9623/82080 [04:12<40:58:50,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9624/82080 [04:14<40:49:56,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9625/82080 [04:16<40:50:43,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9626/82080 [04:18<40:36:43,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9627/82080 [04:20<40:22:13,  2.01s/it]\u001b[A\n",
            "Iteration:  12% 9628/82080 [04:22<40:42:38,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9629/82080 [04:24<40:54:41,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9630/82080 [04:26<40:52:22,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9631/82080 [04:28<40:39:46,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9632/82080 [04:30<41:12:44,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9633/82080 [04:32<41:10:43,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9634/82080 [04:34<41:01:37,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9635/82080 [04:36<40:53:18,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9636/82080 [04:38<41:29:20,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9637/82080 [04:40<41:13:05,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9638/82080 [04:42<41:04:13,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9639/82080 [04:44<40:35:32,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9640/82080 [04:46<40:31:29,  2.01s/it]\u001b[A\n",
            "Iteration:  12% 9641/82080 [04:48<40:53:47,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9642/82080 [04:50<41:10:58,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9643/82080 [04:52<41:01:19,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9644/82080 [04:54<41:14:13,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9645/82080 [04:56<41:15:35,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9646/82080 [04:58<41:00:01,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9647/82080 [05:00<41:10:05,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9648/82080 [05:02<41:10:02,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9649/82080 [05:05<42:00:19,  2.09s/it]\u001b[A\n",
            "Iteration:  12% 9650/82080 [05:07<41:42:22,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9651/82080 [05:09<41:36:36,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9652/82080 [05:11<41:02:21,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9653/82080 [05:13<41:11:22,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9654/82080 [05:15<41:10:41,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9655/82080 [05:17<41:10:23,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9656/82080 [05:19<41:08:34,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9657/82080 [05:21<40:47:11,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9658/82080 [05:23<41:06:32,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9659/82080 [05:25<40:56:04,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9660/82080 [05:27<40:52:03,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9661/82080 [05:29<40:43:47,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9662/82080 [05:31<40:34:25,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9663/82080 [05:33<41:08:39,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9664/82080 [05:35<41:18:19,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9665/82080 [05:37<40:52:55,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9666/82080 [05:39<40:40:36,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9667/82080 [05:41<40:41:08,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9668/82080 [05:43<40:39:52,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9669/82080 [05:45<40:39:41,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9670/82080 [05:47<41:07:53,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9671/82080 [05:49<41:17:56,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9672/82080 [05:51<41:07:51,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9673/82080 [05:53<40:41:35,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9674/82080 [05:56<41:11:45,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9675/82080 [05:58<41:17:04,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9676/82080 [06:00<41:14:06,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9677/82080 [06:02<41:17:09,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9678/82080 [06:04<40:58:30,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9679/82080 [06:06<40:33:22,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9680/82080 [06:08<41:03:30,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9681/82080 [06:10<41:20:00,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9682/82080 [06:12<40:57:39,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9683/82080 [06:14<41:03:01,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9684/82080 [06:16<40:59:03,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9685/82080 [06:18<40:55:52,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9686/82080 [06:20<40:47:32,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9687/82080 [06:22<41:35:08,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9688/82080 [06:24<41:08:20,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9689/82080 [06:26<41:17:04,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9690/82080 [06:28<41:10:15,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9691/82080 [06:30<41:05:01,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9692/82080 [06:32<41:05:19,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9693/82080 [06:34<41:11:39,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9694/82080 [06:36<40:38:13,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9695/82080 [06:38<40:22:39,  2.01s/it]\u001b[A\n",
            "Iteration:  12% 9696/82080 [06:40<40:29:59,  2.01s/it]\u001b[A\n",
            "Iteration:  12% 9697/82080 [06:42<40:35:47,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9698/82080 [06:44<40:41:58,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9699/82080 [06:47<41:05:19,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9700/82080 [06:49<41:00:54,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9701/82080 [06:51<41:09:01,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9702/82080 [06:53<41:02:18,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9703/82080 [06:55<41:04:23,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9704/82080 [06:57<40:41:37,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9705/82080 [06:59<40:33:05,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9706/82080 [07:01<40:54:58,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9707/82080 [07:03<40:35:46,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9708/82080 [07:05<40:40:17,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9709/82080 [07:07<40:59:09,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9710/82080 [07:09<41:09:48,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9711/82080 [07:11<40:48:48,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9712/82080 [07:13<40:53:39,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9713/82080 [07:15<41:02:04,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9714/82080 [07:17<40:47:58,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9715/82080 [07:19<40:55:37,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9716/82080 [07:21<40:41:07,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9717/82080 [07:23<40:39:46,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9718/82080 [07:25<41:08:13,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9719/82080 [07:27<41:03:01,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9720/82080 [07:29<41:16:49,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9721/82080 [07:31<41:17:27,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9722/82080 [07:33<40:43:47,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9723/82080 [07:35<40:37:25,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9724/82080 [07:37<40:08:46,  2.00s/it]\u001b[A\n",
            "Iteration:  12% 9725/82080 [07:39<40:27:01,  2.01s/it]\u001b[A\n",
            "Iteration:  12% 9726/82080 [07:41<40:36:21,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9727/82080 [07:43<40:35:37,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9728/82080 [07:45<40:37:40,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9729/82080 [07:47<40:51:52,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9730/82080 [07:50<41:34:38,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9731/82080 [07:52<41:13:56,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9732/82080 [07:54<41:08:22,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9733/82080 [07:56<40:29:22,  2.01s/it]\u001b[A\n",
            "Iteration:  12% 9734/82080 [07:58<40:35:56,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9735/82080 [08:00<40:24:26,  2.01s/it]\u001b[A\n",
            "Iteration:  12% 9736/82080 [08:02<40:57:34,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9737/82080 [08:04<40:51:38,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9738/82080 [08:06<40:40:13,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9739/82080 [08:08<40:36:39,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9740/82080 [08:10<40:40:39,  2.02s/it]\u001b[A\n",
            "Iteration:  12% 9741/82080 [08:12<40:56:47,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9742/82080 [08:14<41:16:22,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9743/82080 [08:16<41:36:05,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9744/82080 [08:18<41:35:42,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9745/82080 [08:20<41:47:30,  2.08s/it]\u001b[A\n",
            "Iteration:  12% 9746/82080 [08:22<41:27:24,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9747/82080 [08:24<41:27:26,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9748/82080 [08:26<40:52:50,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9749/82080 [08:28<40:44:51,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9750/82080 [08:30<40:50:07,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9751/82080 [08:32<40:47:32,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9752/82080 [08:34<41:02:55,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9753/82080 [08:36<40:47:39,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9754/82080 [08:39<41:28:59,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9755/82080 [08:41<41:32:55,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9756/82080 [08:43<41:04:35,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9757/82080 [08:45<40:50:56,  2.03s/it]\u001b[A\n",
            "Iteration:  12% 9758/82080 [08:47<41:04:35,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9759/82080 [08:49<41:25:44,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9760/82080 [08:51<41:40:18,  2.07s/it]\u001b[A\n",
            "Iteration:  12% 9761/82080 [08:53<41:12:52,  2.05s/it]\u001b[A\n",
            "Iteration:  12% 9762/82080 [08:55<41:19:31,  2.06s/it]\u001b[A\n",
            "Iteration:  12% 9763/82080 [08:57<41:00:39,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9764/82080 [08:59<40:56:48,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9765/82080 [09:01<41:00:32,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9766/82080 [09:03<41:00:51,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9767/82080 [09:05<40:57:27,  2.04s/it]\u001b[A\n",
            "Iteration:  12% 9768/82080 [09:07<41:07:32,  2.05s/it]\u001b[A"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyztGPNAFtnT"
      },
      "source": [
        "rm -rf infobert-roberta-large-anli-full-sl128-lr2e-5-bs16-ts-1-ws1000-wd1e-5-seed42-beta5e-3-alpha5e-3--cl0.5-ch0.9-alr4e-2-amag8e-2-anm0-as3-hdp0.1-adp0-version6/checkpoint-8500"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7bkIScuR8jp",
        "outputId": "e31f574e-4e4c-40cb-f738-7cc43514d67a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 15 06:26:40 2021       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    66W / 149W |      0MiB / 11441MiB |    100%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}